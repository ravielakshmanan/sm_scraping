{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Social Media APIs\n",
    "In this tutorial, we are going to learn to make basic requests from the Facebook and Twitter APIs (application program interface).\n",
    "\n",
    "Each time, we will build out our scripts to perform the same set of actions:\n",
    "\n",
    "1. Print 100-200 posts to the screen (100 is the max for Facebook, Twitter's max is 200)\n",
    "\n",
    "2. Save that data to a CSV file\n",
    "\n",
    "3. Loop our script a few times to increase the number of posts we download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook\n",
    "We will begin with Facebook. Before we write our first script, let's get acquainted with the Graph API Explorer console.\n",
    "\n",
    "For this exercise, we are going to look at posts made from Donald Trump's Facebook page. The user handle for Trump's Facebook page is @DonaldTrump.\n",
    "\n",
    "![Trump's Facebook handle](https://www.dropbox.com/s/y7smhoginpxa72t/trumpfbhandle.jpg?raw=1)\n",
    "\n",
    "### 1. First, get to know the Facebook Graph API\n",
    "* Visit the Facebook Graph API Explorer. This is where you can experiment with the API and see what you get back.\n",
    "    * https://developers.facebook.com/tools/explorer/\n",
    "* (If the Access Token bar is empty, click Get Token.)\n",
    "* Enter the user handle of Facebook account you want to look at, followed by **/posts** (e.g. **donaldtrump/posts**).\n",
    "\n",
    "As you will see, the API returns three or four pieces of information for each post:\n",
    "    1. \"created_time\"\n",
    "    2. \"message\" (if present)\n",
    "    3. \"story\" (if present)\n",
    "    3. \"id\"\n",
    "\n",
    "*This information is useful, but there is a lot more available to us. We just have to tell the API what we want. Just a few of the additional fields are illustrated here:*\n",
    "\n",
    "![Examples of Graph API syntax](https://www.dropbox.com/s/pmucwa2tm70i0e5/fbgraphnames.jpg?raw=1)\n",
    "\n",
    "### 2. Add some additional fields to your search\n",
    "* To get more specific in your search by adding additional syntax. Take a look at the documentation about Facebook posts. The Fields table lists all of the pieces of information the API can return about individual Facebook posts. The syntax you need is in the Name column (e.g. the documentations tells us that 'from' returns \"Information (name and id) about the Profile that created the Post.\").\n",
    "    * https://developers.facebook.com/docs/graph-api/reference/v3.0/post\n",
    "* You update your search by adding **?fields=** followed by the syntax from the Name column of the Fields table, e.g.\n",
    "    * donaldtrump/posts***?fields=created_time,from,message,link,shares,comments***\n",
    "* Return to the Graph API Explorer console and do a new search containing additional fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Writing our first script to interact with the Facebook Graph API: Print a user's last 100 posts to the screen\n",
    "In this script, we are going to:\n",
    "\n",
    "* Ask the Facebook Graph API for the last 100 posts, then...\n",
    "   * Tell Python to go through each one and look for text in the message field, then...\n",
    "        * Print the text in the message field to the screen (if it exists)\n",
    "        \n",
    "**TASKS:**\n",
    "\n",
    "1) Copy the Access Token from the Graph API Explorer and paste it between the quote marks after **access_token** in the code below.\n",
    "\n",
    "2) Enter the ID of the account you would like to look at between the speech marks after **publisher_id**, e.g. 'donaldtrump', 'nytimes', 'breitbart'\n",
    "\n",
    "Run the code in the cell using SHIFT + ENTER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tell Python we want to use the Requests library (http://docs.python-requests.org/en/master/)\n",
    "import requests\n",
    "\n",
    "# Copy the Access Token from the Graph API Explorer console\n",
    "access_token = ''\n",
    "# Enter the Facebook handle of the page you want to scrape, e.g. 'nytimes' or 'Breitbart'\n",
    "publisher_id = 'donaldtrump'\n",
    "\n",
    "# Tell the API want fields you want it to return (copy everything after 'fields=' from the Graph API console)\n",
    "# Possible fields for posts can be found here: https://developers.facebook.com/docs/graph-api/reference/v3.0/post/\n",
    "post_fields = 'created_time,from,message,link,shares,comments'\n",
    "\n",
    "# Construct the Facebook Graph API URL to be opened\n",
    "graph_url = 'https://graph.facebook.com/v3.0/' + publisher_id + '/posts/?fields=%s&limit=100&access_token=%s' % (post_fields, access_token)\n",
    "# Print the Graph API URL to the screen (not necessary - just useful for checking in browser)\n",
    "print (graph_url)\n",
    "\n",
    "# Get the contents of URL using Requests\n",
    "r = requests.get(graph_url)\n",
    "# Extract the JSON from the Graph page\n",
    "json = r.json()\n",
    "# Identify the JSON pertaining to the ~100 Facebook posts returned by the API\n",
    "posts = json['data']\n",
    "\n",
    "# Go through each of the posts returned by the API\n",
    "for post in posts:\n",
    "    # For each post, display the text ('message') followed by a new line to make the text easier to read ('\\n')\n",
    "    try:\n",
    "        post_text = post['message']\n",
    "        print(post_text + '\\n')\n",
    "    except:\n",
    "        print('No message \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... BONUS: with a couple more lines of very similar code you can get the text of the comments\n",
    "With a very slight alteration to the code at the bottom of the script above, we can change our script so that it prints the first batch of comments below the post instead. The process it goes through is:\n",
    "\n",
    "* Ask the Facebook Graph API for the first 100 posts, then...\n",
    "   * Tell Python to go through each one and look for comments, then...\n",
    "        * Ask the Facebook Graph API for the first page of comment replies (if any exist), then...\n",
    "            * Print the text in the comment message field to the screen\n",
    "            \n",
    "Again, paste your Access Token into the **access_token** variable.\n",
    "\n",
    "Then run the code using SHIFT + ENTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell Python we want to use the Requests library\n",
    "import requests\n",
    "\n",
    "# Copy the Access Token from the Graph API Explorer console\n",
    "access_token = ''\n",
    "# Enter the Facebook handle of the page you want to scrape, e.g. 'nytimes' or 'Breitbart'\n",
    "publisher_id = 'donaldtrump'\n",
    "\n",
    "# Tell the API want fields you want it to return (copy everything after 'fields=' from the Graph API console)\n",
    "# Possible fields for posts can be found here: https://developers.facebook.com/docs/graph-api/reference/v2.9/post/\n",
    "post_fields = 'created_time,from,message,link,shares,comments'\n",
    "\n",
    "# Construct the Facebook Graph API URL to be opened\n",
    "graph_url = 'https://graph.facebook.com/v3.0/' + publisher_id + '/posts/?fields=%s&limit=100&access_token=%s' % (post_fields, access_token)\n",
    "# Print the Graph API URL to the screen (not necessary - just useful for checking in browser)\n",
    "print (graph_url)\n",
    "\n",
    "# Open and read the URL using Requests\n",
    "r = requests.get(graph_url)\n",
    "# Extract the JSON from the Graph page\n",
    "json = r.json()\n",
    "# Identify the JSON pertaining to the ~100 Facebook posts returned by the API\n",
    "posts = json['data']\n",
    "\n",
    "# Go through each of the posts returned by the API\n",
    "for post in posts:\n",
    "    # THIS IS WHERE THE NEW CODE STARTS\n",
    "    # Find the JSON for the comments (if any exist)\n",
    "    try:\n",
    "        # Extract the first set of comments and assign them to a variable named 'comments'\n",
    "        comments = post['comments']['data']\n",
    "        \n",
    "        # Go through each of the comments\n",
    "        for comment in comments:\n",
    "            \n",
    "            comment_text = comment['message']\n",
    "            print(comment_text + '\\n')\n",
    "    except:\n",
    "        print('No comments \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...Now let's add a little extra code to our first script so the user's posts get saved to a CSV file\n",
    "Viewing posts on screen is fine, but chances are you are going to want to download data from the API so you can perform some kind of analysis.\n",
    "\n",
    "Python has a module that makes it really easy to save stuff to CSV files. These can then be viewed with spreadsheet software (Google Sheers, Excel, Numbers, etc), text editors, etc.\n",
    "\n",
    "The code below builds upon our first script and creates a CSV file containing the 'created_time' (timestamp), 'link' and 'message' of each post to a CSV file named **'facebook_posts.csv'**\n",
    "\n",
    "This file will be saved to the directory from which you are running this notebook, e.g. Desktop\n",
    "\n",
    "This script is going to:\n",
    "\n",
    "* Create a file named 'facebook_posts.csv' and initiate Python's CSV writer\n",
    "* Ask the Facebook Graph API for the first 100 posts\n",
    "   * Tell Python to go through each one and look for a 'created_time', 'link' and 'message', then...\n",
    "            * Bundle those three ('created_time', 'link' and 'message') together into a variable we'll call 'post_info', then...\n",
    "            * Add the contents of that 'post_info' variable to our CSV and print it to screen\n",
    "   * Finally, it will close the CSV file\n",
    "   \n",
    "You know the drill:\n",
    "* Paste your Access Token, then run the code (SHIFT + ENTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'csv' module so we can easily write to a CSV file\n",
    "import csv\n",
    "# Import the 'requests' module to interact with web pages and the 'string' module to manipulate strings of text\n",
    "import requests\n",
    "\n",
    "# Copy your Access Token from the Graph API Explorer: https://developers.facebook.com/tools/explorer\n",
    "access_token = ''\n",
    "\n",
    "# Create an output file, name it and give it write privileges ('w')\n",
    "csv_file = open('facebook_posts.csv', 'w')\n",
    "# Initiate the CSV writer\n",
    "writer = csv.writer(csv_file)\n",
    "\n",
    "# Enter the Facebook handle of the page you want to scrape, e.g. 'nytimes', 'Breitbart', etc.\n",
    "publisher_id = 'donaldtrump'\n",
    "\n",
    "# Enter the post fields you want to be returned by the Facebook API (copy these from the API Graph Explorer)\n",
    "post_fields = 'created_time,from,message,link,shares,comments'\n",
    "\n",
    "# Construct the Facebook Graph API URL to be opened\n",
    "graph_url = 'https://graph.facebook.com/v3.0/' + publisher_id + '/posts/?fields=%s&limit=100&access_token=%s' % (post_fields, access_token)\n",
    "# Print your graph URL\n",
    "print (graph_url)\n",
    "\n",
    "# Open and read your graph URL using Requests\n",
    "r = requests.get(graph_url)\n",
    "# Extract the JSON from the Graph page\n",
    "json = r.json()\n",
    "# Identify the JSON pertaining to the ~100 Facebook posts returned by the API\n",
    "posts = json['data']\n",
    "\n",
    "# Extract the data we want from every post returned by the API\n",
    "for post in posts:\n",
    "    # Extract the post's timestamp\n",
    "    timestamp = post['created_time']\n",
    "    \n",
    "    # Extract the post's id\n",
    "    try:\n",
    "        link = post['link']\n",
    "    except:\n",
    "        link = 'No link'\n",
    "    \n",
    "    # Extract the 'message' text from the post\n",
    "    try:\n",
    "        message = post['message']\n",
    "    except:\n",
    "        message = 'No message'\n",
    "        \n",
    "    post_info = timestamp, link, message\n",
    "\n",
    "    # Write the contents of the 'post_info' to the output file\n",
    "    writer.writerow(post_info)\n",
    "    # Print the contents to screen so we can see what we're getting\n",
    "    print(post_info)\n",
    "\n",
    "# Close the CSV file you have been writing to\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...finally, lets add a little code so the script goes through multiple pages of results\n",
    "It's possible that you will want to download more than the 100 posts you get on the first page of results.\n",
    "\n",
    "Conveniently, the last thing the API tells us, at the bottom of the page, is the URL of the next page of results in ['paging']['next']\n",
    "\n",
    "![The address of the next of results](https://www.dropbox.com/s/znvk1ah5mtbxoe9/paging.jpg?raw=1)\n",
    "\n",
    "One (slightly clunky) way of working through multiple pages of results is to use a for loop.\n",
    "\n",
    "When the code below gets to the final result of a page, it looks for the URL of the next page of results. It then changes the graph_url variable to contain that URL, so that when the loop returns to the beginning, it is using this new URL.\n",
    "\n",
    "The code below repeats itself three times AKA it goes through three pages of results, resulting in 300 posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'csv' module so we can easily write to a CSV file\n",
    "import csv\n",
    "# Import the 'requests' module to interact with web pages and the 'string' module to manipulate strings of text\n",
    "import requests\n",
    "\n",
    "# Copy your Access Token from the Graph API Explorer: https://developers.facebook.com/tools/explorer\n",
    "access_token = ''\n",
    "\n",
    "# Create an output file, name it and give it write privileges ('w')\n",
    "csv_file = open('facebook_posts_large.csv', 'w')\n",
    "# Initiate the CSV writer\n",
    "writer = csv.writer(csv_file)\n",
    "\n",
    "# Enter the Facebook handle of the page you want to scrape, e.g. 'nytimes', 'Breitbart', etc.\n",
    "publisher_id = 'donaldtrump'\n",
    "\n",
    "# Enter the post fields you want to be returned by the Facebook API (copy these from the API Graph Explorer)\n",
    "post_fields = 'created_time,from,message,link,shares,comments'\n",
    "\n",
    "# Construct the initial Facebook Graph API URL to be opened\n",
    "graph_url = 'https://graph.facebook.com/v3.0/' + publisher_id + '/posts/?fields=%s&limit=100&access_token=%s' % (post_fields, access_token)\n",
    "\n",
    "# Set a loop to repeat three times\n",
    "for i in range(0, 3):\n",
    "    # Print your graph URL\n",
    "    print (graph_url)\n",
    "\n",
    "    # Open and read your graph URL using Requests\n",
    "    r = requests.get(graph_url)\n",
    "    # Extract the JSON from the Graph page\n",
    "    json = r.json()\n",
    "    # Identify the JSON pertaining to the ~100 Facebook posts returned by the API\n",
    "    posts = json['data']\n",
    "    # THIS IS NEW CODE\n",
    "    # Identify the JSON pertaining to the URL of the next page of results and call it 'next_page'\n",
    "    next_page = json['paging']['next']\n",
    "\n",
    "    # Extract the data we want from every post returned by the API\n",
    "    for post in posts:\n",
    "        # Extract the post's timestamp\n",
    "        timestamp = post['created_time']\n",
    "\n",
    "        # Extract the post's id\n",
    "        try:\n",
    "            link = post['link']\n",
    "        except:\n",
    "            link = 'No link'\n",
    "\n",
    "        # Extract the 'message' text from the post\n",
    "        try:\n",
    "            message = post['message']\n",
    "        except:\n",
    "            message = 'No message'\n",
    "\n",
    "        post_info = timestamp, link, message\n",
    "\n",
    "        # Write the contents of the 'post_info' to the output file\n",
    "        writer.writerow(post_info)\n",
    "        \n",
    "        # THIS IS NEW CODE\n",
    "        # Update the URL in the graph_url variable so it becomes the one we called 'next_page'\n",
    "        # When the loop restarts it will use this updated address to gather data from the \n",
    "        graph_url = next_page\n",
    "\n",
    "# Close the CSV file you have been writing to\n",
    "csv_file.close()\n",
    "\n",
    "print(\"Finished downloading posts!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter\n",
    "Let's turn our attention to Twitter.\n",
    "\n",
    "To use the Twitter API, we need to create an app:\n",
    "\n",
    "* Go to https://apps.twitter.com\n",
    "* Click Create New App\n",
    "* Fill out the Name, Description and Website fields, agree to the Developer Agreement and click Create your Twitter application\n",
    "\n",
    "Once your app has been created...\n",
    "\n",
    "* Click the 'Keys and Access Tokens' tab\n",
    "* We are going to need:\n",
    "    * Consumer Key (API Key)\n",
    "    * Consumer Secret (API Secret)\n",
    "\n",
    "### 1. Take a look at what comes back from the API\n",
    "Unlike Facebook, Twitter no longer has an API console for us to play around with.\n",
    "\n",
    "However, there are a number of Python modules that will do the heavy lifting for you and make it very easy to gather tweets.\n",
    "\n",
    "We are going to use Twython. (https://twython.readthedocs.io/en/latest/)\n",
    "\n",
    "Like Facebook, the data that comes back from the Twitter API is a long stream of JSON, some of which is nested (e.g. \"hashtags\" is nested within \"entities\").\n",
    "\n",
    "* **Have a look at a sample response for a user's tweets:** https://github.com/twitterdev/tweet-updates/blob/master/samples/initial/compatibility_extended_13996.json\n",
    "\n",
    "*N.B. If you want to view the documentation for the Twitter API it is here: https://developer.twitter.com/en/docs/api-reference-index*\n",
    "\n",
    "*The docs for Get Tweets timeline, which we are interacting with via Twython, are here: https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-user_timeline*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use the Twython library to print 200 tweets\n",
    "We are going to use the script below to get a user's last 200 tweets.\n",
    "\n",
    "Twython has made it so easy for us that we can get a user's tweets back with about 10 lines of code, most of which is copied directly from Twython's support page.\n",
    "\n",
    "1) Paste your Consumer Key (API Key) between the speech marks after **APP_KEY**\n",
    "\n",
    "2) Paste your Consumer Secret (API Secret) between the speech marks after **APP_SECRET**\n",
    "\n",
    "3) Enter the Twitter handle of the account you want to look at between the speech marks after **user_id**, e.g. 'realdonaldtrump', 'nytimes', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from twython import Twython\n",
    "\n",
    "# Paste the Consumer Key (API Key) from apps.twitter.com\n",
    "APP_KEY = ''\n",
    "# Paste the Consumer Secret (API Secret) from apps.twitter.com\n",
    "APP_SECRET = ''\n",
    "\n",
    "# The following three lines are lifted from the https://twython.readthedocs.io/en/latest/usage/starting_out.html#oauth2\n",
    "# Full disclosure: I don't understand what they do, but they work, which is all that matters\n",
    "twitter = Twython(APP_KEY, APP_SECRET, oauth_version=2)\n",
    "ACCESS_TOKEN = twitter.obtain_access_token()\n",
    "twitter = Twython(APP_KEY, access_token=ACCESS_TOKEN)\n",
    "\n",
    "# Enter the Twitter handle of the account you want to look at\n",
    "user_id = 'realdonaldtrump'\n",
    "\n",
    "# Get JSON containing the user's 200 most recent tweets\n",
    "tweets = twitter.get_user_timeline(screen_name=user_id, count=200, tweet_mode='extended')\n",
    "\n",
    "# Go through each of the 200 tweets in 'tweets' JSON\n",
    "for tweet in tweets:\n",
    "    # Print the text of the tweet followed by a new line ('\\n)\n",
    "    print(tweet['full_text'] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... Now, let's save those tweets to a CSV file again\n",
    "Just as we did with the Facebook posts, we are going to save these 200 tweets to a CSV file.\n",
    "\n",
    "This time the file name is **tweets.csv**\n",
    "\n",
    "The resulting CSV should contain each of the following for each tweet:\n",
    "* The timestamp ('created_at')\n",
    "* The ID of the tweet ('id')\n",
    "* The contents of the tweet ('full_text')\n",
    "* The number of retweets the tweet has received ('retweet_count')\n",
    "* The number of timee the tweet has been favorited ('favorite_count')\n",
    "\n",
    "N.B. Don't forget to fill out the **APP_KEY** and **APP_SECRET** variables again. You can also change **user_id** if you want to get tweets from a different account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from twython import Twython\n",
    "\n",
    "# Paste the Consumer Key (API Key) from apps.twitter.com\n",
    "APP_KEY = ''\n",
    "# Paste the Consumer Secret (API Secret) from apps.twitter.com\n",
    "APP_SECRET = ''\n",
    "\n",
    "# These three lines are lifted from the https://twython.readthedocs.io/en/latest/usage/starting_out.html#oauth2\n",
    "# Full disclosure: I don't understand what they do, but they work, which is all that matters\n",
    "twitter = Twython(APP_KEY, APP_SECRET, oauth_version=2)\n",
    "ACCESS_TOKEN = twitter.obtain_access_token()\n",
    "twitter = Twython(APP_KEY, access_token=ACCESS_TOKEN)\n",
    "\n",
    "# Enter the Twitter handle of the account you want to look at\n",
    "user_id = 'realdonaldtrump'\n",
    "\n",
    "# Get JSON containing the user's 200 most recent tweets\n",
    "tweets = twitter.get_user_timeline(screen_name=user_id, count=200, tweet_mode='extended')\n",
    "\n",
    "# Create an output file, name it and give it write privileges ('w')\n",
    "csv_file = open('tweets.csv', 'w')\n",
    "# Initiate the CSV writer\n",
    "writer = csv.writer(csv_file)\n",
    "\n",
    "# Go through each of the 200 tweets in 'tweets' JSON\n",
    "for tweet in tweets:\n",
    "    # Extract the timestamp of the tweet\n",
    "    timestamp = tweet['created_at']\n",
    "    # Extract the ID of the tweet\n",
    "    tweet_id = tweet['id']\n",
    "    # Extract the text of the tweet\n",
    "    text = tweet['full_text']\n",
    "    # Extract the number of retweets\n",
    "    retweets = tweet['retweet_count']\n",
    "    # Extract the number of favourites the tweet has got\n",
    "    favourites = tweet['favorite_count']\n",
    "    \n",
    "    tweet_info = timestamp, tweet_id, text, retweets, favourites\n",
    "    \n",
    "    # Write the contents of the 'post_info' to the output file\n",
    "    writer.writerow(tweet_info)\n",
    "    # Print the contents to screen so we can see what we're getting\n",
    "    print(tweet_info)\n",
    "\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally... let's download thousands of tweets instead of hundreds\n",
    "The Twitter API lets you collect up to 3,200 tweets in one session.\n",
    "\n",
    "We know we can get up to 200 at a time, so we just need to repeat that request 16 times (16 * 200 = 3,200).\n",
    "\n",
    "We're going to do that with a simple for loop that repeats itself 16 times.\n",
    "\n",
    "The resulting tweets are going to be saved to a file named **tweets_long.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from twython import Twython\n",
    "\n",
    "# Paste the Consumer Key (API Key) from apps.twitter.com\n",
    "APP_KEY = ''\n",
    "# Paste the Consumer Secret (API Secret) from apps.twitter.com\n",
    "APP_SECRET = ''\n",
    "\n",
    "# These three lines are lifted from the https://twython.readthedocs.io/en/latest/usage/starting_out.html#oauth2\n",
    "# I don't understand what they do, but they work, which is all that matters\n",
    "twitter = Twython(APP_KEY, APP_SECRET, oauth_version=2)\n",
    "ACCESS_TOKEN = twitter.obtain_access_token()\n",
    "twitter = Twython(APP_KEY, access_token=ACCESS_TOKEN)\n",
    "\n",
    "# Create an output file, name it and give it write privileges ('w')\n",
    "csv_file = open('tweets_long.csv', 'w')\n",
    "# Initiate the CSV writer\n",
    "writer = csv.writer(csv_file)\n",
    "\n",
    "# Enter the Twitter handle of the account you want to look at\n",
    "user_id = 'realdonaldtrump'\n",
    "\n",
    "# Find the most recent tweet in the user's timeline\n",
    "latest_tweet = twitter.get_user_timeline(screen_name=user_id, count=1)\n",
    "# Get the ID of the latest tweet\n",
    "tweet_id = latest_tweet[0]['id']\n",
    "\n",
    "# Set up a loop that will repeat 16 times\n",
    "for i in range(0, 16):\n",
    "    # Get JSON containing the user's 200 most recent tweets\n",
    "    tweets = twitter.get_user_timeline(screen_name=user_id, count=200, tweet_mode='extended', max_id=tweet_id)\n",
    "    \n",
    "    # Go through each of the 200 tweets in 'tweets' JSON\n",
    "    for tweet in tweets:\n",
    "        # Extract the timestamp of the tweet\n",
    "        timestamp = tweet['created_at']\n",
    "        # Extract the ID of the tweet\n",
    "        tweet_id = tweet['id']\n",
    "        # Extract the text of the tweet\n",
    "        text = tweet['full_text']\n",
    "        # Extract the number of retweets\n",
    "        retweets = tweet['retweet_count']\n",
    "        # Extract the number of favourites the tweet has got\n",
    "        favourites = tweet['favorite_count']\n",
    "\n",
    "        # Group the scraped tweet information together in a variable called tweet_info\n",
    "        tweet_info = timestamp, tweet_id, text, retweets, favourites\n",
    "\n",
    "        # Write the contents of the 'post_info' to the output file\n",
    "        writer.writerow(tweet_info)\n",
    "        # Print the contents to screen so we can see what we're getting\n",
    "        print(tweet_info)\n",
    "        \n",
    "        tweet_id = tweet['id'] - 1\n",
    "\n",
    "# Close the CSV file you have been writing to\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
